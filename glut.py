import os
import gc
from mmgp import offload
import torch
import numpy as np
import gradio as gr
import socket
import psutil
import random
import argparse
import datetime
from diffusers import ZImagePipeline, ZImageTransformer2DModel
from transformers import Qwen3Model


parser = argparse.ArgumentParser() 
parser.add_argument("--server_name", type=str, default="127.0.0.1", help="IPåœ°å€ï¼Œå±€åŸŸç½‘è®¿é—®æ”¹ä¸º0.0.0.0")
parser.add_argument("--server_port", type=int, default=7891, help="ä½¿ç”¨ç«¯å£")
parser.add_argument("--share", action="store_true", help="æ˜¯å¦å¯ç”¨gradioå…±äº«")
parser.add_argument("--mcp_server", action="store_true", help="æ˜¯å¦å¯ç”¨mcpæœåŠ¡")
parser.add_argument("--compile", action="store_true", help="æ˜¯å¦å¯ç”¨compileåŠ é€Ÿ")
parser.add_argument("--res_vram", type=int, default=2000, help="æ˜¾å­˜ä¿ç•™ï¼Œå•ä½MBã€‚æ•°å€¼è¶Šå¤§ï¼Œå ç”¨æ˜¾å­˜è¶Šå°ï¼Œé€Ÿåº¦è¶Šæ…¢")
args = parser.parse_args()

print(" å¯åŠ¨ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾… bilibili@åå­—é±¼ https://space.bilibili.com/893892")
print(f'\033[32mPytorchç‰ˆæœ¬ï¼š{torch.__version__}\033[0m')
if torch.cuda.is_available():
    device = "cuda" 
    print(f'\033[32mæ˜¾å¡å‹å·ï¼š{torch.cuda.get_device_name()}\033[0m')
    total_vram_in_gb = torch.cuda.get_device_properties(0).total_memory / 1073741824
    print(f'\033[32mæ˜¾å­˜å¤§å°ï¼š{total_vram_in_gb:.2f}GB\033[0m')
    mem = psutil.virtual_memory()
    print(f'\033[32må†…å­˜å¤§å°ï¼š{mem.total/1073741824:.2f}GB\033[0m')
    if torch.cuda.get_device_capability()[0] >= 8:
        print(f'\033[32mæ”¯æŒBF16\033[0m')
        dtype = torch.bfloat16
    else:
        print(f'\033[32mä¸æ”¯æŒBF16ï¼Œä»…æ”¯æŒFP16\033[0m')
        dtype = torch.float16
else:
    print(f'\033[32mCUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥\033[0m')
    device = "cpu"

os.makedirs("outputs", exist_ok=True)
repo_id = "./models/Z-Image-Turbo"
budgets = int(torch.cuda.get_device_properties(0).total_memory/1048576 - args.res_vram)
stop_generation = False

text_encoder = offload.fast_load_transformers_model(
    f"{repo_id}/text_encoder/mmgp.safetensors",
    do_quantize=False,
    modelClass=Qwen3Model,
    forcedConfigPath=f"{repo_id}/text_encoder/config.json",
)
transformer = offload.fast_load_transformers_model(
    f"{repo_id}/transformer/mmgp.safetensors",
    do_quantize=False,
    modelClass=ZImageTransformer2DModel,
    forcedConfigPath=f"{repo_id}/transformer/config.json",
)
pipe = ZImagePipeline.from_pretrained(
    repo_id, 
    text_encoder=text_encoder,
    transformer=transformer,
    torch_dtype=dtype,
    low_cpu_mem_usage=False, 
)
#text_encoder._dtype = dtype 
mmgp = offload.all(
    pipe, 
    pinnedMemory= ["text_encoder", "transformer"],
    budgets={'*': budgets}, 
    extraModelsToQuantize = ["text_encoder"],
    compile=True if args.compile else False,
)
"""offload.save_model(
    model=pipe.transformer, 
    file_path=f"{repo_id}/transformer/mmgp.safetensors", 
    config_file_path=f"{repo_id}/transformer/config.json",
)
offload.save_model(
    model=pipe.text_encoder, 
    file_path=f"{repo_id}/text_encoder/mmgp.safetensors", 
    #config_file_path=f"{repo_id}/text_encoder/config.json",
)"""

# è§£å†³å†²çªç«¯å£ï¼ˆæ„Ÿè°¢licyké…±æä¾›çš„ä»£ç ~ï¼‰
def find_port(port: int) -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        if s.connect_ex(("localhost", port)) == 0:
            print(f"ç«¯å£ {port} å·²è¢«å ç”¨ï¼Œæ­£åœ¨å¯»æ‰¾å¯ç”¨ç«¯å£...")
            return find_port(port=port + 1)
        else:
            return port
        

def exchange_width_height(width, height):
    return height, width, "âœ… å®½é«˜äº¤æ¢å®Œæ¯•"


def stop_generate():
    global stop_generation
    stop_generation = True
    return "ğŸ›‘ ç­‰å¾…ç”Ÿæˆä¸­æ­¢"


def scale_resolution_1_5(width, height):
    """
    å°†å®½åº¦å’Œé«˜åº¦éƒ½æ”¾å¤§1.5å€ï¼Œå¹¶æŒ‰ç…§16çš„å€æ•°å‘ä¸‹å–æ•´
    """
    new_width = int(width * 1.5) // 16 * 16
    new_height = int(height * 1.5) // 16 * 16
    return new_width, new_height, "âœ… åˆ†è¾¨ç‡å·²è°ƒæ•´ä¸º1.5å€"


def generate(
    prompt, 
    width, 
    height, 
    num_inference_steps, 
    batch_images, 
    seed_param, 
):
    global stop_generation
    results = []
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    prompt_embeds, _ = pipe.encode_prompt(prompt)
    for i in range(batch_images):
        if stop_generation:
            stop_generation = False
            yield results, f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œæœ€åç§å­æ•°{seed+i-1}"
            break
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"outputs/{timestamp}.png"
        output = pipe(
            height=height,
            width=width,
            num_inference_steps=num_inference_steps, 
            guidance_scale=0.0, 
            generator=torch.Generator().manual_seed(seed+i),
            prompt_embeds=prompt_embeds,
        )
        image = output.images[0]
        image.save(filename)
        results.append(image)
        yield results, f"ç§å­æ•°{seed+i}ï¼Œä¿å­˜åœ°å€{filename}"
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
    

with gr.Blocks(title="Z-Image-diffusers", theme=gr.themes.Soft(font=[gr.themes.GoogleFont("IBM Plex Sans")])) as demo:
    gr.Markdown("""
            <div>
                <h2 style="font-size: 30px;text-align: center;">Z-Image-diffusers</h2>
            </div>
            <div style="text-align: center;">
                åå­—é±¼
                <a href="https://space.bilibili.com/893892">ğŸŒbilibili</a> 
                |Z-Image-diffusers
                <a href="https://github.com/gluttony-10/Z-Image-diffusers">ğŸŒgithub</a> 
            </div>
            <div style="text-align: center; font-weight: bold; color: red;">
                âš ï¸ è¯¥æ¼”ç¤ºä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä½“éªŒä½¿ç”¨ã€‚
            </div>
            """)
    
    with gr.Tabs():
        with gr.TabItem("Z-Image-diffusers"):
            with gr.Row():
                with gr.Column():
                    prompt = gr.Textbox(label="æç¤ºè¯", placeholder="è¯·è¾“å…¥æç¤ºè¯...")
                    generate_button = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary', scale=4)
                    with gr.Accordion("å‚æ•°è®¾ç½®", open=True):
                        with gr.Row():
                            width = gr.Slider(label="å®½åº¦", minimum=256, maximum=2048, step=16, value=1024)
                            height = gr.Slider(label="é«˜åº¦", minimum=256, maximum=2048, step=16, value=1024)
                        with gr.Row():
                            exchange_button = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜")
                            scale_1_5_button = gr.Button("1.5å€åˆ†è¾¨ç‡")
                        batch_images = gr.Slider(label="æ‰¹é‡ç”Ÿæˆ", minimum=1, maximum=100, step=1, value=1)
                        num_inference_steps = gr.Slider(label="é‡‡æ ·æ­¥æ•°ï¼ˆæ¨è9æ­¥ï¼‰", minimum=1, maximum=100, step=1, value=9)
                        seed_param = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥è‡ªç„¶æ•°ï¼Œ-1ä¸ºéšæœº", value=-1)
                with gr.Column():
                    info = gr.Textbox(label="æç¤ºä¿¡æ¯", interactive=False)
                    image_output = gr.Gallery(label="ç”Ÿæˆç»“æœ", interactive=False)
                    stop_button = gr.Button("ä¸­æ­¢ç”Ÿæˆ", variant="stop")
        
    gr.on(
        triggers=[generate_button.click, prompt.submit],
        fn = generate,
        inputs = [
            prompt,
            width,
            height,
            num_inference_steps,
            batch_images,
            seed_param,
        ],
        outputs = [image_output, info]
    )
    exchange_button.click(
        fn=exchange_width_height, 
        inputs=[width, height], 
        outputs=[width, height, info]
    )
    scale_1_5_button.click(
        fn=scale_resolution_1_5,
        inputs=[width, height],
        outputs=[width, height, info]
    )
    stop_button.click(
        fn=stop_generate, 
        inputs=[], 
        outputs=[info]
    )

if __name__ == "__main__": 
    demo.launch(
        server_name=args.server_name, 
        server_port=find_port(args.server_port),
        share=args.share, 
        mcp_server=args.mcp_server,
        inbrowser=True,
    )